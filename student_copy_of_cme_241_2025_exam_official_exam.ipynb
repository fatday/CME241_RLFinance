{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatday/CME241_RLFinance/blob/main/student_copy_of_cme_241_2025_exam_official_exam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20PTIXvHGn89"
      },
      "source": [
        "# Stanford CME 241 (Winter 2025) - Exam"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions"
      ],
      "metadata": {
        "id": "x6FxUmpMdxQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We recommend taking this exam using Google Colab. The first step to do right now is to copy this notebook by clicking \"File → Save A Copy in Drive\" at the top left. You will be able to edit the copy that opens in a new tab.**\n",
        "\n",
        "Welcome to the exam for *Foundations of Reinforcement Learning with Applications in Finance*. Here are instructions for taking the exam:\n",
        "\n",
        "- There is a 72 hour exam period in which you can take the exam. We only expect the exam to take about 3-6 hours, but we want to allow extra time to minimize stress. The Exam Period begins Friday, Feb 28th at 8:30 PM PST and ends at the deadline: Monday, March 3rd at 8:30 PM PST. You must submit your exam by the deadline.\n",
        "- There are 2 problems worth a total of 30 points. You can use the Table of Contents on the left navigation bar (click the icon that looks like a bulleted list) to quickly move between the problems and examine the number of points for each.\n",
        "- Put all of your writing, math formulas, code, graphs etc. in your copy of this Google Colab.\n",
        "- Show work! Explain your thoughts! This helps with partial credit.\n",
        "- To submit, print your copy of this Google Colab to PDF using File -> Print. Submit the PDF to the `Exam` assignment on Gradescope. Finally, share your copy of this Google Colab (via the \"Share\" button at the upper right) with `neelsn@stanford.edu`.\n",
        "- Do not share or upload this exam or any documents derived from it to any public platform.\n",
        "- Please use your time wisely and balance with the appropriate number of points in the questions.\n",
        "- You will be graded mainly on correctness, though partial credit will be awarded so please explain your process and intutition where appropriate. Where code is expected, please write code that is easy to read and understand (i.e. well-commented and typed).\n",
        "- If you have any trouble during the exam, please email `neelsn@stanford.edu`.\n",
        "- Ed will be disabled during the 72 hour exam period. Send any questions/clarifications to `neelsn@stanford.edu`."
      ],
      "metadata": {
        "id": "hyVmHz7Yd-jW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Honor Code\n",
        "\n",
        "**Please note the following:**\n",
        "\n",
        "- We trust that you will follow [The Stanford Honor Code](https://communitystandards.stanford.edu/policies-and-guidance/honor-code).\n",
        "- Consulting with any other humans, students or otherwise, is <ins>NOT</ins> allowed. This exam is to be taken individually.\n",
        "- ChatGPT/Claude/Gemini/any other LLMs are <ins>NOT</ins> allowed. We have run each of these problems through many LLMs and know what answers they produce. If you do not know the answer, try your best; do not use LLMs for this exam in any capacity. Again, we can tell when they have been used. Also, as a word of caution, LLMs have gotten these problems wrong across many different prompts; the exam was designed in a manner that would trip up LLMs, hence the faulty answers provided by them.\n",
        "\n",
        "Inputting your information below acts as acknowledgement of your adherence to the Stanford Honor Code and the rules of this exam. Failure to comply will result in disciplinary action and a 0 on the exam.\n",
        "\n",
        "**Name:** Put Your Name Here\n",
        "\n",
        "**SUNet ID:** Put Your SUNET ID Here\n",
        "\n",
        "**Colab URL:** Put the URL to this Colab Here"
      ],
      "metadata": {
        "id": "S0Rbme4Vl-VJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Setup"
      ],
      "metadata": {
        "id": "vfn7BEgmBHB7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a few steps to take to set up this notebook.\n",
        "\n",
        "First, connect to a hosted runtime by clicking the Connect tab in the top right and proceed with the exam. Once you do this, you should see a green checkmark with the words \"RAM\" and \"Disk\" next to it. If you already see this, you're already connected, and can skip this step. (Note that there is a small chance runtimes disconnect in the middle of the 4-hour exam. If so, you should be able to connect again by clicking \"Reconnect\". You may then click on the menu item \"Runtime → Run Before\" to rerun all cells and recover progress.)\n",
        "\n",
        "This exam is setup as a self-contained Colab/Jupyter notebook that clones the RL book git repository. All libraries within RL book as well as the external libraries that are required are automatically downloaded in this Jupyter notebook.\n",
        "\n",
        "The following shell script command (with the ! prefix) enables this. Make sure you run this cell just one time, as it takes a minute or so to get all libraries downloaded. After you have run the cell, the rest of the jupyter notebook can be run repeatedly as you make your changes (avoid running this cell a second time so your repeated runs are quick enough).\n",
        "\n",
        "If you have issues running it, immediately reach out to `neelsn@stanford.edu`."
      ],
      "metadata": {
        "id": "Esr4rl6DdrIt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdIJKYwjwWzL"
      },
      "outputs": [],
      "source": [
        "# Clone the rl-book repository\n",
        "!git clone https://github.com/TikhonJelvis/rl-book.git\n",
        "\n",
        "# Change the working directory to the rl-book directory\n",
        "%cd rl-book\n",
        "\n",
        "# Move to the branch with proper installation requirements\n",
        "!git checkout notebook\n",
        "!pip install -r notebooks/notebook-requirements.txt\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from typing import Tuple, Callable, Iterator\n",
        "import numpy as np\n",
        "from math import comb\n",
        "from dataclasses import dataclass\n",
        "from rl.distribution import SampledDistribution\n",
        "from rl.markov_process import State, NonTerminal, Terminal\n",
        "from rl.markov_decision_process import MarkovDecisionProcess\n",
        "from pprint import pprint\n",
        "from rl.policy import DeterministicPolicy\n",
        "from rl.distribution import Constant\n",
        "from itertools import islice\n",
        "from rl.function_approx import LinearFunctionApprox\n",
        "from rl.approximate_dynamic_programming import value_iteration\n",
        "from rl.approximate_dynamic_programming import ValueFunctionApprox\n",
        "from rl.iterate import converged"
      ],
      "metadata": {
        "id": "RkOwfx_1xUPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH9UodnPGn8-"
      },
      "source": [
        "## Question 1: Racing Optimization (20 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXTfjw8FGn8-"
      },
      "source": [
        "### Problem Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l42VQugucgBe"
      },
      "source": [
        "Let's say you are running a 400m race. You have to blend speed and stamina. If you go too fast for too long, you will run out of stamina and risk having to go too slow due to too little stamina. If you go too slow for too long, you might not be able to make up for too much time consumed. You have to find the best balance of speed and stamina conservation. Let us formalize this problem.\n",
        "\n",
        "Let the race length be $D$ meters. Let the total stamina at the start be $S$. When your remaining stamina is $s$, you can run with a maximum effort of $e_{max}(s)$ where $e_{max}$ is a non-decreasing function, with $e_{max}(0) = 0$. To be clear, when your stamina is $s$, you make a choice to run with an effort $e$ where $0 \\leq e \\leq e_{max}(s)$. When you run with an effort $e$, your speed $v$ is a random variable (due to variable wind and track conditions) defined as $v = \\max(0, w)$ where $w$ follows a normal distribution with mean $f(e)$ and standard deviation $\\alpha \\cdot f(e)$ where $f$ is a non-decreasing function, with $f(0) = 0$, and $\\alpha \\geq 0$ is a constant (for simplicity, assume speed is constant in each time step and that speed can be instantaneously changed from one time step to the next). When at stamina $s$, running with effort $e$ and speed $v$ for time $\\Delta t$ depletes your stamina by $\\beta \\cdot e \\cdot v \\cdot \\Delta t$ for some constant $\\beta > 0$. Find the optimal effort-application strategy at any point in the race to minimize the total time to run this race.  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1 (6 Points)"
      ],
      "metadata": {
        "id": "IYBiF9LirBul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us start by modeling this problem as an MDP. Let us discretize time in $\\Delta t$ seconds intervals with each time step indexed by $t = 0, 1, 2, \\ldots$\n",
        "\n",
        "1. First, let's determine the state space. We want to model the states as tuples of length $2$. Model the state space. Don't forget to indicate the terminal state(s).\n",
        "\n",
        "2. Let's next determine the possible actions. In each state, what actions can be taken?\n",
        "\n",
        "3. What is the reward when the agent transitions from one state to the next (knowing that ultimately you want to minimze the total time to run the entire race)? *Hint: The reward is not constant across all states. You could run out of stamina or finish the race in the middle of a timestep, which creates cases for the way the reward function is represented.*"
      ],
      "metadata": {
        "id": "qC0cpqberf_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer\n",
        "\n",
        "1. **FILL IN**\n",
        "2. **FILL IN**\n",
        "3. **FILL IN**"
      ],
      "metadata": {
        "id": "yIndeD0QuDQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2 (6 Points)\n",
        "\n",
        "We have provided starter code below. Your task is to fill in the `sample_next_state_reward()` and the `actions()` functions below. The class is defined in a way that makes it easy to instantiate, as you will see in future parts; as such, do not modify any code outside of what is directed here. After you modify the code in the cell below, run the next cells. Only modify code where you are told."
      ],
      "metadata": {
        "id": "bzQkkxStvrXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FloatPair = Tuple[float, float]\n",
        "\n",
        "HIGH_NEGATIVE = -1e6\n",
        "TRANSITION_SAMPLES = 300\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class SpeedAndStamina(MarkovDecisionProcess[FloatPair, float]):\n",
        "  # total distance to be run in the race\n",
        "  D: float\n",
        "  # total stamina available at the start of the race\n",
        "  S: float\n",
        "  # number of discrete action-choices (evenly-spaced)\n",
        "  A: int\n",
        "  # time-interval for each time step\n",
        "  delta_t: float\n",
        "  # function defining max effort, given remaining stamina\n",
        "  e_max: Callable[[float], float]\n",
        "  # function defining mean speed, given effort\n",
        "  f: Callable[[float], float]\n",
        "  # parameter alpha defining stdev of speed as a fraction of mean speed\n",
        "  alpha: float\n",
        "  # parameter beta defining stamina depletion, given effort and speed\n",
        "  beta: float\n",
        "\n",
        "  def step(\n",
        "    self,\n",
        "    state: NonTerminal[FloatPair],\n",
        "    e: float\n",
        "  ) -> SampledDistribution[Tuple[State[FloatPair], float]]:\n",
        "    d, s = state.state\n",
        "    mean = self.f(e)\n",
        "\n",
        "    def sample_next_state_reward() -> Tuple[State[FloatPair], float]:\n",
        "      next_state = _\n",
        "      reward = _\n",
        "\n",
        "      '''\n",
        "      FILL IN\n",
        "      '''\n",
        "\n",
        "      return next_state, reward\n",
        "\n",
        "    return SampledDistribution(\n",
        "        sampler=sample_next_state_reward,\n",
        "        expectation_samples=TRANSITION_SAMPLES\n",
        "    )\n",
        "\n",
        "  def actions(self, state: NonTerminal[FloatPair]) -> Iterator[float]:\n",
        "    '''\n",
        "    FILL IN\n",
        "    '''\n",
        "    _, s = state.state\n",
        "    max_action = _ # fill this in (only modify this line)\n",
        "    return ((i + 1) * max_action / self.A for i in range(self.A))"
      ],
      "metadata": {
        "id": "0FckK0tbwwDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def e_max(s: float) -> float:\n",
        "    E_max = 40.0   # Saturation level\n",
        "    s0 = 50.0      # Midpoint stamina\n",
        "    k = 0.1        # Steepness\n",
        "    return E_max / (1.0 + np.exp(-k * (s - s0)))\n",
        "\n",
        "stamina_vals = np.linspace(0, 100, 101)\n",
        "effort_vals = [e_max(s) for s in stamina_vals]\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(stamina_vals, effort_vals, '-o')\n",
        "plt.title(r\"Logistic $e_{\\max}(s) = \\frac{40}{1 + e^{-0.1(s - 50)}}$\")\n",
        "plt.xlabel(\"Stamina (s)\")\n",
        "plt.ylabel(\"Max Effort (e_max)\")\n",
        "plt.xlim(100,0)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9pyUWIZkyRES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Now, let's create an instance of this problem, and a simple policy such that the\n",
        "effort chosen is a constant fraction (call it \\beta) of the maximum effort\n",
        "$e_{max}(s)$ allowed at any state.\n",
        "'''\n",
        "\n",
        "D: float = 400.0\n",
        "S: float = 100.0\n",
        "A: int = 10\n",
        "delta_t: float = 1\n",
        "e_max: Callable[[float], float] = lambda s: 40.0/(1.0+np.exp(-0.1*(s-50.0)))\n",
        "f: Callable[[float], float] = lambda y: y\n",
        "alpha: float = 0.1\n",
        "beta: float = 0.025\n",
        "\n",
        "sas = SpeedAndStamina(\n",
        "  D=D,\n",
        "  S=S,\n",
        "  A=A,\n",
        "  delta_t=delta_t,\n",
        "  e_max=e_max,\n",
        "  f=f,\n",
        "  alpha=alpha,\n",
        "  beta=beta\n",
        ")\n",
        "\n",
        "beta = 0.25\n",
        "policy_func = lambda state: beta * e_max(state[1])\n",
        "\n",
        "dp = DeterministicPolicy(action_for=policy_func)\n",
        "\n",
        "traces = sas.action_traces(Constant(value=NonTerminal((D, S))), dp)\n",
        "\n",
        "num_traces = 1000\n",
        "\n",
        "times = []\n",
        "staminas = []\n",
        "for trace in islice(traces, num_traces):\n",
        "  elems = list(trace)\n",
        "  last_d, last_s = elems[-1].next_state.state\n",
        "  if last_d == 0:\n",
        "      times.append(-sum(x.reward for x in elems))\n",
        "      staminas.append(last_s)\n",
        "\n",
        "print(f\"There were {len(times)} finishes, average time for finishes was \\\n",
        "{np.mean(times):.2f} and the average finishing stamina was \\\n",
        "{np.mean(staminas):.2f}\")"
      ],
      "metadata": {
        "id": "qQFSnlQjxYSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your average time for finishes is around $80$ and your average finishing stamina is around $35$, you're on the right track! Great job!"
      ],
      "metadata": {
        "id": "BF9y0cu3ytwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3 (2 Points)\n",
        "\n",
        "Next, implement the approximate value iteration algorithm. We have defined feature functions, and you only need to fill in the `done_func()` function.\n",
        "\n",
        "Don't be worried if it takes a long time to run. Mine takes 9 minutes, and the times can vary. However, if executing this cell takes longer than 20 minutes, you probably have a mistake."
      ],
      "metadata": {
        "id": "-L5cwLQe27hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_STATE_SAMPLES = 300\n",
        "ERROR_THRESHOLD = 0.2\n",
        "\n",
        "ffs = [\n",
        "    lambda _: 1.0,\n",
        "    lambda state: state.state[0],\n",
        "    lambda state: state.state[1],\n",
        "    lambda state: state.state[0] * state.state[0],\n",
        "    lambda state: state.state[1] * state.state[1],\n",
        "    lambda state: state.state[0] * state.state[1]\n",
        "]\n",
        "vf_approx_init = LinearFunctionApprox.create(ffs)\n",
        "\n",
        "uniform = SampledDistribution(\n",
        "    sampler=lambda: NonTerminal((np.random.uniform(0, D), np.random.uniform(0, S)))\n",
        ")\n",
        "\n",
        "vi = value_iteration(\n",
        "    mdp=sas,\n",
        "    γ=1,\n",
        "    approx_0=vf_approx_init,\n",
        "    non_terminal_states_distribution=uniform,\n",
        "    num_state_samples=NUM_STATE_SAMPLES\n",
        ")\n",
        "\n",
        "def done_func(fa1: ValueFunctionApprox[FloatPair], fa2: ValueFunctionApprox[FloatPair]) -> bool:\n",
        "  distances = np.linspace(0, D, 10)\n",
        "  staminas = np.linspace(0, S, 10)\n",
        "  '''\n",
        "  FILL IN\n",
        "  Replace the underscore with your code\n",
        "  '''\n",
        "  return max(_) < ERROR_THRESHOLD\n",
        "\n",
        "opt_vf = converged(vi, done=done_func)"
      ],
      "metadata": {
        "id": "IfPGvkWexivF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 4 (2 Points)\n",
        "\n",
        "Now let's extract the Optimal Policy from this Optimal Value Function. Fill in the code below."
      ],
      "metadata": {
        "id": "3mm03CjJCOXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def opt_policy(state: FloatPair) -> float:\n",
        "  def q_value(e: float) -> float:\n",
        "    '''\n",
        "    FILL IN\n",
        "    '''\n",
        "    return _\n",
        "  return max(sas.actions(NonTerminal(state)), key=q_value)\n",
        "\n",
        "policy = DeterministicPolicy(action_for=opt_policy)"
      ],
      "metadata": {
        "id": "J3nQbEjUy18g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 5 (1 Point)\n",
        "\n",
        "Run the following cell to generate a heatmap and comment on the heatmap. Does the color grading make sense? Is there anything that doesn't make intuitive sense? If so, comment on why this might be the case."
      ],
      "metadata": {
        "id": "FIEw3MZg5qdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_vals = np.linspace(0, D, 10)\n",
        "y_vals = np.linspace(0, S, 10)\n",
        "Y, X = np.meshgrid(y_vals, x_vals)\n",
        "all_times = [[-opt_vf(NonTerminal((d, s))) for s in y_vals] for d in x_vals]\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.pcolormesh(X, Y, all_times, shading='auto', cmap='viridis')\n",
        "plt.colorbar(label=\"Time Taken\")\n",
        "plt.xlabel(\"Distance Left\")\n",
        "plt.ylabel(\"Stamina Left\")\n",
        "plt.title(\"Optimal Time Taken as a function of distance and stamina left\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IdhxWYjTCaOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer\n",
        "\n",
        "**FILL IN**"
      ],
      "metadata": {
        "id": "fWmyPagVQSYW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 6 (1 Point)\n",
        "\n",
        "Explain the impact of feature functions. What do they do? Why are they relevant in approximate value iteration, and why are the relevant in this problem? What would happen if we added more feature functions?"
      ],
      "metadata": {
        "id": "xabUn3kR24fJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer\n",
        "\n",
        "**FILL IN**"
      ],
      "metadata": {
        "id": "QhOyAaEs3hK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 7 (1 Point)\n",
        "\n",
        "Run the code below and comment on the graphs for the varying stamina levels."
      ],
      "metadata": {
        "id": "YPzWKhNz4Cc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stamina_levels = [0, 10, 25, 50, 75, 100]\n",
        "distance_values = np.linspace(0, D, 50)\n",
        "plt.figure(figsize=(8, 6))\n",
        "for s_fixed in stamina_levels:\n",
        "    speeds = []\n",
        "    for d_val in distance_values:\n",
        "        e = policy.act(NonTerminal((d_val, s_fixed))).value\n",
        "        speed = sas.f(e)\n",
        "        speeds.append(speed)\n",
        "    plt.plot(distance_values, speeds, label=f\"Stamina = {s_fixed}\")\n",
        "\n",
        "plt.xlabel(\"Distance Left\")\n",
        "plt.ylabel(\"Speed (mean)\")\n",
        "plt.title(\"Agent Speed vs. Distance Left for Various Stamina Levels\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gT-P7wE5Cezf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer\n",
        "\n",
        "**FILL IN**"
      ],
      "metadata": {
        "id": "3leOHwzD4o6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 8 (1 Point)\n",
        "\n",
        "For the final part of this problem, we want to think about different ways to solve this problem using RL. Which RL algorithm do you think is best suited to solve this problem? Why? A few, detailed sentences is fine."
      ],
      "metadata": {
        "id": "8UfBqJg7OMWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer\n",
        "\n",
        "**FILL IN**"
      ],
      "metadata": {
        "id": "b4-hpbPZQrv8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2: Let's Play a Game! (10 Points)"
      ],
      "metadata": {
        "id": "fuCAVQ-6i624"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem Setup\n",
        "\n",
        "You want to design a profitable and sticky quiz game. It's you asking questions versus the players answering questions. You are thinking of 2 games.\n",
        "\n",
        "**Game 1**: To play a round, a player has to pay you \\\\$10. There are 10 questions in a round. If a player answers 1 question correctly, he wins \\\\$1.  But if he gets 7, 8 or 9 questions correct, he wins an extra \\\\$10 bonus. For example, if he gets 8 answers correct, he wins 8 + 10 = \\\\$18, if he answers 9 answers correctly, he wins 9+10= \\\\$19. But if he answers all 10 questions correctly, he wins \\\\$100.\n",
        "\n",
        "Also, if a player wins at least \\\\$5 or more from a round, he will play one more time. If he wins less than \\\\$5 from a round, he gets eliminated. The game ends as soon as the player gets eliminated. Note that there is no other termination condition for this game other than a round where the player wins less than \\$5.\n",
        "\n",
        "**Game 2**: A player pays you \\$10 to play each round. There are 10 questions in a round. If a player answers at least 8 of the 10 questions right, he wins $5^2=25$ dollars and goes to the second round. Otherwise (if he answers less than 8 questions right), he gets eliminated. If he answers at least 8 questions right in the second round, he wins $5^3=125$ dollars and goes to the third round. Otherwise (if he answers less than 8 questions right), he gets eliminated. If he answers at least 8 questions right in the third round, he wins $5^4=625$ dollars and goes to the fourth round. Likewise, he wins $5^5=3125$ in round four if he answers at least 8 questions right (basically, the prize money multiplies 5 times in each round). The game ends as soon as the player gets eliminated or after 4 rounds are completed.\n",
        "\n",
        "If a large number of players play this game, how much money are you expected to make in each of the two games, assuming that the probability of a random player answering any single question correctly is 50% (in any round, in each of the two games)?\n",
        "\n",
        "Which of these two games is more profitable for you (on an expected basis)?\n",
        "\n",
        "Hint: You can model these two games as Markov Reward Processes (MRPs), and evaluate the Value Function for these MRPs."
      ],
      "metadata": {
        "id": "fCSzxZFTQTkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1 (1 Point)"
      ],
      "metadata": {
        "id": "R6fW0-1p6jAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $p_i$, where $0 \\leq i \\leq 10$, denote the probability of a random player answering exactly $i$ questions correctly in any round. What is $p_i$ equal to? Note that we've assumed that the probability of answering each question correctly is 50%."
      ],
      "metadata": {
        "id": "C1qS2TC4T47N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer\n",
        "\n",
        "**FILL IN**"
      ],
      "metadata": {
        "id": "lw6-Hs5MQV62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2 (2 Points)"
      ],
      "metadata": {
        "id": "7Q5RDxpBQmQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Game 1, let us denote $R_i$ as the money you have to give to a player in any round when the player answers $i$ questions correctly. We want to define an infinite-horizon MRP (in terms of $R_i$ and $p_i$, $0 \\leq i \\leq 10$), whose Value Function gives us the Expected Gains for you in Game 1.\n",
        "\n",
        "1. What is the state space and the state-transition probability function of this infinite-horizon MRP?\n",
        "2. What is the reward function?\n",
        "3. Write the Bellman Equation for this MRP.\n",
        "4. Solve for the Value Function."
      ],
      "metadata": {
        "id": "xFNPJnHQQspb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer\n",
        "\n",
        "1. **FILL IN**\n",
        "2. **FILL IN**\n",
        "3. **FILL IN**\n",
        "4. **FILL IN**"
      ],
      "metadata": {
        "id": "ELu9XaFURp5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have provided starter code for you to use to evaluate the Value Function you solved above. Fill in the V_game1 expression to solve for the expected gains."
      ],
      "metadata": {
        "id": "reTaUJcoR82g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# q: probability of getting a question right\n",
        "q = 0.5 # assume equal probability of right or wrong answer\n",
        "# p: probability of player answering exactly i questions correct in any single\n",
        "# round\n",
        "'''\n",
        "Define p as an array using list comprehension so that we have\n",
        "probabilities for all values of i; fill in the list comprehension with your\n",
        "previously derived mathematical expression. This might be helpful:\n",
        "https://www.w3schools.com/python/ref_math_comb.asp\n",
        "Only fill in the underscore with code; keep the list comprehension as is\n",
        "'''\n",
        "p = [_ for i in range(11)]\n",
        "# R: list of prize amounts the player recieves for each possible number of\n",
        "# correct answers from 0 to 10\n",
        "'''\n",
        "This list should be hardcoded and should be of length 11; define it such\n",
        "that R[0] is the prize if the player gets 0 correct, R[8] is the prize if the\n",
        "player gets 8 correct, etc.\n",
        "'''\n",
        "R = [0, 1, 2, 3, 4, 5, 6, 17, 18, 19, 100]\n",
        "# V1: the expected gains for you (from each player) in Game 1\n",
        "V_game1 = _\n",
        "print(f\"Expected Gains for you (from each player) in Game 1 is {V_game1:.2f}\")"
      ],
      "metadata": {
        "id": "r2siMK67SX2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3 (2 Points)\n",
        "We want to generalize Game 2 such that:\n",
        "\n",
        "-  Instead of winning $5^{j+1}$ in round $j$, the player wins $c^{j+1}$ for some arbitrary positive real number $c$.\n",
        "- Instead of needing to answer a minimum of 8 questions right to get to the next round (and winning a reward), the player needs to answer a minimum of $m$ questions right, for some arbitrary integer $0 \\leq m \\leq 10$.\n",
        "- Instead of the game terminating after 4 rounds, the game terminates after $n$ rounds, for some arbitrary integer $n \\geq 1$.\n",
        "\n",
        "Unlike Game 1, Game 2 is a finite-horizon MRP. Here our state space is $\\{1, 2, \\ldots, n\\}$. The state at the start of round $j$ is $j$, for all $1 \\leq j \\leq n$. Let $V_j$ denote the Value Function for state $j$.\n",
        "\n",
        "As we have always handled with finite-horizon problems, you can work your way backwards in time (backward induction method for solving the Value Function of a finite-horizon MRP). Start by working out the expression for $V_n$ (note: the game terminates after the $n$-th round, no matter how many questions are answered in round $n$). Next, write $V_j$ in terms of $V_{j+1}$, $p_i$ and $c$, for all $1 \\leq j \\leq n-1$ ($p_i$ is the probability that the player gets $i$ questions right in any round). This is essentially the MRP Bellman Equation. To work out the MRP Bellman Equation (just like in Game 1), you need the state transition probability function and the reward function. But you have to be careful in taking into account premature termination in each round (if the player answers less than $m$ questions right). It will be useful for you to define $P_m = \\sum_{i=m}^{10} p_i$, which is the probability of moving to the next round.\n",
        "\n",
        "\n",
        "\n",
        "Fill in the following two expressions:"
      ],
      "metadata": {
        "id": "Usudz-h9qM49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer\n",
        "\n",
        "1. $V_n = \\_$\n",
        "2. $V_j=\\_ \\mbox{ for all } 1 \\leq j \\leq n - 1$ (MRP Bellman Equation)"
      ],
      "metadata": {
        "id": "ehBbJ-0wcnCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 4 (3 Points)\n",
        "\n",
        "Using the MRP Bellman Equation you worked out above, derive a closed-form expression for $V_j$. To achieve this, do repeated substitutions, or use induction, to eliminate $V_{j+1}, V_{j+2}, \\dots$. You should end up with a summation that explicitly depends on $j,n,P_m$, and $c$. In your final answer, show $V_j$ purely as a function of these parameters without any remaining recursion.\n",
        "\n",
        "**Hint: Unroll or Use a Pattern**\n",
        "\n",
        "   Once you have a recursive relationship—something like  \n",
        "   $$V_j = (\\text{some constant}) + (\\text{some probability}) \\times (\\text{next } V_{j+1}) - \\dotsb$$  \n",
        "   you can either  \n",
        "   1. Do *repeated substitution*, eliminating $V_{j+1}, V_{j+2}, \\dots$ until you reach $V_n$.  \n",
        "   2. Or *guess* a summation expression for $V_j$ (involving powers of the pass probability and the prize cost) and then prove by induction that it satisfies your recursion.  \n",
        "\n",
        "   Look for a *geometric-series*-like pattern involving the pass probability each time the player advances. Be methodical in tracking how many times you subtract the cost $c$-term and how many times you still collect \\$10."
      ],
      "metadata": {
        "id": "Pgig-RUoraP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer\n",
        "\n",
        "1. $V_j=\\_ \\mbox{ for all } 1 \\leq j \\leq n$\n",
        "\n",
        "Note that the final answer you are seeking for Game 2 is $V_1$"
      ],
      "metadata": {
        "id": "AfKrzNY1w_DP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 5 (1 Point)\n",
        "\n",
        "We define parameters below. Fill in the code below to implement the value function you solved above."
      ],
      "metadata": {
        "id": "0ZMWLigp1BDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c = 5\n",
        "m = 8\n",
        "n = 4\n",
        "\n",
        "Pm = sum(p[m:])\n",
        "'''\n",
        "FILL IN\n",
        "'''\n",
        "V_game2 = _\n",
        "print(f\"Expected Gains for you (from each player) in Game 2 is {V_game2:.2f}\")"
      ],
      "metadata": {
        "id": "7ZPaRv1-z5Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 6 (1 Point)\n",
        "\n",
        "Which game would you rather play? Why?"
      ],
      "metadata": {
        "id": "h9UJ3o6Z2rMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer\n",
        "\n",
        "**FILL IN**"
      ],
      "metadata": {
        "id": "v7QZn1Ym2xli"
      }
    }
  ]
}